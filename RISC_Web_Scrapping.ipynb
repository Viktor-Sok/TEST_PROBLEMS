{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get install chromium-chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # this module helps in web scrapping.\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(journal_name):\n",
    "    '''\n",
    "    Chooses correct html element from journal_name list \n",
    "    based on font color\n",
    "    '''\n",
    "    name_index = None\n",
    "    for index, name in enumerate(journal_name):\n",
    "        if (\"font color=\\\"#F26C4F\\\"\" in str(name)):\n",
    "            name_index = index\n",
    "    return name_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_publication_activity_page(link, driver_1):\n",
    "    '''\n",
    "    Retrievs id,  journal name and number of publications\n",
    "    per each year from publication activity page\n",
    "    '''\n",
    "    driver_1.get(link)\n",
    "    soup1 = BeautifulSoup(driver_1.page_source, 'html.parser')\n",
    "    \n",
    "    # Extracting id fro the link\n",
    "    data_from_publication_activity_page = []\n",
    "    id_ = int(link.split('=')[-1])\n",
    "    data_from_publication_activity_page.append(id_)\n",
    "    \n",
    "    # Getting the journal name\n",
    "    journal_name = soup1.find_all('a',href= re.compile('^title_about'))\n",
    "    index_of_required_tag = get_index(journal_name) # chooses correct html element from journal_name\n",
    "    data_from_publication_activity_page.append(journal_name[index_of_required_tag].text)\n",
    "    \n",
    "    # Getting number of publications per eahc year in the range from 2011 to 2020\n",
    "    font_tag = soup1.find(\"font\", text = \"Число статей в РИНЦ\")\n",
    "    td_tag = font_tag.parent # parent of font tag above\n",
    "    first_table_row = td_tag.parent\n",
    "    soup1 = BeautifulSoup(str(first_table_row), 'html.parser') # put in html string of first row for parsing\n",
    "    cells_of_first_row = soup1.find_all('td')[2:] # [2:] excludes first two cells\n",
    "    for cell in cells_of_first_row:\n",
    "        '''\n",
    "        appends number of publications per year\n",
    "        '''\n",
    "        data_from_publication_activity_page.append(int(cell.text))\n",
    "                                                       \n",
    "    return data_from_publication_activity_page\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заполнение формы для поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://elibrary.ru/titles.asp' # url address of the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing user agent to avoid captcha\n",
    "options = Options()\n",
    "ua = UserAgent()\n",
    "userAgent = ua.random\n",
    "options.add_argument(f'user-agent={userAgent}')\n",
    "options.add_argument(\"--headless\") # comment out to see the browser window\n",
    "driver = webdriver.Chrome(options=options, executable_path=r\"/usr/lib/chromium-browser/chromedriver\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_dropdown = Select(driver.find_element_by_name(\"countryid\"))\n",
    "country_dropdown.select_by_value(\"RUS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dropdown = Select(driver.find_element_by_name(\"language\"))\n",
    "language_dropdown.select_by_value(\"RU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_dropdown = Select(driver.find_element_by_name(\"rubriccode\"))\n",
    "rubric_dropdown.select_by_value(\"200000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_css_selector(\"div[onclick='title_search()']\").click() # click on search button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of journals found\n",
    "html = driver.page_source\n",
    "soup_n = BeautifulSoup(html, 'html.parser')\n",
    "n_journals = int(soup_n.find(\"td\", class_ = \"redref\").b.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цикл по всем страницам результата поиска для сбора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>n_papers_risc_2011</th>\n",
       "      <th>n_papers_risc_2012</th>\n",
       "      <th>n_papers_risc_2013</th>\n",
       "      <th>n_papers_risc_2014</th>\n",
       "      <th>n_papers_risc_2015</th>\n",
       "      <th>n_papers_risc_2016</th>\n",
       "      <th>n_papers_risc_2017</th>\n",
       "      <th>n_papers_risc_2018</th>\n",
       "      <th>n_papers_risc_2019</th>\n",
       "      <th>n_papers_risc_2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, name, n_papers_risc_2011, n_papers_risc_2012, n_papers_risc_2013, n_papers_risc_2014, n_papers_risc_2015, n_papers_risc_2016, n_papers_risc_2017, n_papers_risc_2018, n_papers_risc_2019, n_papers_risc_2020]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating empty dataframe to store results\n",
    "years = ['n_papers_risc_' + str(year) for year in range(2011, 2021)]\n",
    "result_table = pd.DataFrame(columns = ['id', 'name'] + years)\n",
    "result_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_of_pages = (n_journals % 100) + 1 # there are 100 journals on the page\n",
    "#driver_1 = webdriver.Chrome(\"/usr/lib/chromium-browser/chromedriver\") # another instance of the browser\n",
    "rate = [i/10 for i in range(1,11)] # list for randomizing our request rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_of_pages):\n",
    "    # changing user agent to avoid captcha\n",
    "    userAgent = ua.random\n",
    "    options.add_argument(f'user-agent={userAgent}')\n",
    "    options.add_argument(\"--headless\") # comment out to see the browser window\n",
    "    driver_1 = webdriver.Chrome(options=options, executable_path=r\"/usr/lib/chromium-browser/chromedriver\")\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    links = []\n",
    "    for link in soup.find_all('a',href=True, title = \"Анализ публикационной активности журнала\"):  # in html anchor/link is represented by the tag <a>\n",
    "        links.append('https://elibrary.ru/'+link.get('href')) \n",
    "    for link in links:\n",
    "        temp_list = get_data_from_publication_activity_page(link, driver_1)\n",
    "        result_table = result_table.append(pd.DataFrame([temp_list], columns = ['id', 'name'] + years), ignore_index = True)\n",
    "        time.sleep(random.choice(rate)) # Avoiding web scraping detection by randomizing our request rate to closely mimic human interaction\n",
    "    driver_1.quit()\n",
    "    try:\n",
    "        driver.find_element_by_css_selector(\"a[title='Следующая страница']\").click() # click on next page button\n",
    "    except NoSuchElementException:\n",
    "        break\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit() # cleaning up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table.to_excel(\"Informatics_Journals_RISC.xlsx\", index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table.to_csv('Informatics_Journals_RISC.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd0752c97cc6bc111c51b9bd047092464b0f155495a5f7c2778f8b038adb90cf9af"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
